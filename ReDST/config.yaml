lr: 0.005
momentum: 0.9
lr_decay_rate: 0.1
batch_size: 500
hidden_size: 100
epoch: 50

input_type: "binary" # options: "prob", "binary"
use_lbp: False # whether use lbp graph, options: True, False
use_lbp_new: False # whether use lbp graph, options: True, False
use_lbp_update: False
use_lbp_anneal: True
mu: 10
lbp_iters: 1 # num of iterations to propagation on the graph
lbp_alpha: 0.5 # weight for last_belief
lbp_beta: 0.2 # weight for the propogation 
prop_beta_turn2last: 0.2 # propogation weight from turn_belief to last_belief
prop_beta_last2turn: 0 # propogation weight from last_belief to turn_belief 
agg_type: "concat" # how to aggregate the ori_fea and propagated_fea, options: "concat", "none"

dst_model: "RNN" # options: "RNN", "MLP"
rnn_cell: "GRU" # options: "GRU", "LSTM"
use_which: "pred" # use which input to do reasoning, options: "pred", "grd"

rnn_dropout: 0.4
rnn_hidden_size: 500

mlp_dropout: 0.2
mlp_hidden_size: 500

dump_res: True
win_size: 3
